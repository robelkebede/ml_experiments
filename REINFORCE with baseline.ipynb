{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765005bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import collections,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d340c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    \n",
    "    def __init__(self,lr=0.01):\n",
    "        \n",
    "        self.state = []\n",
    "        self.ob_space = env.observation_space.shape\n",
    "        self.action_space = env.action_space.n\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        \n",
    "        #state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "        \n",
    "        self.model.add(tf.keras.layers.InputLayer(input_shape=(self.ob_space[0],)))\n",
    "        self.model.add(tf.keras.layers.Dense(self.action_space, activation=\"softmax\"))\n",
    "        \n",
    "        #self.loss = tf.keras.metrics.mean_squared_error()\n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.var_list_fn = [i for i in self.model.trainable_weights]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,state):\n",
    "        \n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def update(self,state,target,action):\n",
    "\n",
    "        \n",
    "   \n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            tape.watch(self.model.variables)\n",
    "        \n",
    "            output = tf.squeeze(self.model(state))\n",
    "       \n",
    "            self.picked_action_prob = tf.gather(output, action)\n",
    "            \n",
    "            self.loss = -tf.math.log(self.picked_action_prob) * target\n",
    "            \n",
    "        grads = tape.gradient(self.loss,self.model.variables)\n",
    "        \n",
    "        self.opt.apply_gradients(zip(grads, self.var_list_fn))\n",
    "\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58eff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Policy()\n",
    "\n",
    "\n",
    "test = np.zeros(4).reshape(1,-1)\n",
    "\n",
    "state = np.zeros_like(env.observation_space.shape[0]).reshape(1,-1)\n",
    "target = np.zeros_like(env.action_space.n)\n",
    "action = np.zeros_like(env.action_space.n)\n",
    "\n",
    "act = np.array(0)\n",
    "\n",
    "ad = np.array([[1.0330247,1.0195254,1.0393808,0.9419981]])\n",
    "\n",
    "print(p.update(test,ad,act))\n",
    "#print(p.predict(test))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50156cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value():\n",
    "    def __init__(self,lr=0.01):\n",
    "        \n",
    "        self.ob_space = env.observation_space.shape\n",
    "        \n",
    "        self.model = tf.keras.models.Sequential()\n",
    "                \n",
    "        self.model.add(tf.keras.layers.InputLayer(input_shape=(self.ob_space[0],)))\n",
    "        self.model.add(tf.keras.layers.Dense(self.ob_space[0]))\n",
    "        \n",
    "        self.var_list_fn = [i for i in self.model.trainable_weights]\n",
    "        \n",
    "        self.opt = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def predict(self,state):\n",
    "        \n",
    "        return self.model.predict(state)\n",
    "    \n",
    "    def update(self,state,target):\n",
    "        \n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "        \n",
    "            tape.watch(self.model.variables)\n",
    "        \n",
    "            output = self.model(state)\n",
    "            \n",
    "            self.loss = tf.keras.metrics.mean_squared_error(target,output)\n",
    "            #self.loss = output * target\n",
    "            \n",
    "            \n",
    "        grads = tape.gradient(self.loss,self.model.variables)\n",
    "        \n",
    "        self.opt.apply_gradients(zip(grads, self.var_list_fn))\n",
    "    \n",
    "        \n",
    "        return self.loss\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros(4).reshape(1,-1)\n",
    "\n",
    "v = Value()\n",
    "\n",
    "print(v.update(test,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424cd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
    "    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    log_t = []\n",
    "    log_i = []\n",
    "    \n",
    "    \n",
    "    for i in range(num_episodes): # bun episode\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        episode = []\n",
    "        \n",
    "        log_t.append([])\n",
    "        log_i.append([])\n",
    "        \n",
    "        r_ = 0\n",
    "        \n",
    "        for t in itertools.count():\n",
    "\n",
    "            # take action based on the currnet random policy \n",
    "            \n",
    "            action_probs = estimator_policy.predict(state.reshape(1,-1))\n",
    "            action = np.random.choice(np.arange(len(action_probs.reshape(-1))), p=action_probs.reshape(-1)) # is this the same as argmax \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            print(f\"step =>{action} reward =>{reward} iter =>{t}\")\n",
    "            \n",
    "            # hold the output of the step in a list (s,a,r,n_s,d)\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            \n",
    "            \n",
    "            r_ += reward\n",
    "            log_t[i].append(r_)\n",
    "            log_i[i].append(t)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        for i,transition in enumerate(episode):\n",
    "            total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:])) # episode[t:] == episode[-1]\n",
    "            \n",
    "            # compute the advantage \n",
    "            \n",
    "            baseline_value = estimator_value.predict(transition.state.reshape(1,-1))  \n",
    "            advantage = total_return - baseline_value\n",
    "            \n",
    "            # update value est and policy est \n",
    "            \n",
    "                        \n",
    "            estimator_value.update(transition.state.reshape(1,-1), total_return)\n",
    "            estimator_policy.update(transition.state.reshape(1,-1), advantage, transition.action)\n",
    "        \n",
    "    return log_t,log_i\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "value = Value()\n",
    "re = reinforce(env,policy,value,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_t,log_i = re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c5d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=1)\n",
    "legend = [\"ep \"+str(i) for i in range(10)]\n",
    "\n",
    "\n",
    "\n",
    "sns.lineplot(data = log_t)\n",
    "plt.legend(legend, ncol=2, loc='upper left');\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    env.reset()\n",
    "    for _ in range(1000):\n",
    "        #env.render()\n",
    "        a = env.action_space.sample()\n",
    "        print(a)\n",
    "        env.step(a) # take a random action\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
